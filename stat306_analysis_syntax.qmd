---
title: "stat306_project_analysis_syntax"
format: html
editor: visual
---

# Exploratory & Statistical Anlsysis

## 1. Data Cleaning

1.  Each observation in the original dataset `boston.txt` is split across two lines, we preprocessed the data by `readLines()` and combine every two lines into one rows.
2.  The original dataset contains 22 rows of metadata and no headers, we skipped 22 rows when reading the data and we named it `data` and added headers afterwards using `colnames(data)`
3.  we then use `any(is.na(data))` to check if there is any NA in any rows, and there is no NA in our data.

Thus we have our cleaned dataset of `data`

```{r}
# Read the file as raw lines
lines <- readLines("boston.txt")

# Combine every two lines into one row
merged_lines <- sapply(seq(1, length(lines), by = 2), function(i) {
  paste(trimws(lines[i]), trimws(lines[i + 1]))
})

# read data and add column names 
data <- read.table(text = merged_lines, skip = 22,header = FALSE)
colnames(data) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS",
                    "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV")
# View the head of data
head(data)
any(is.na(data))
```

## 2. Summary Statistics

1.  we used `describe` function from `psych` package to present the summary statistics of the columns of the dataset in a table format
2.  The output of `describe` is a table with rows as the variables in our data and columns representing the summary statistics of each variables. In this table, we mainly looks at the `mean`, `sd`, `min`, `max`, `range`, and there are no particular irregular patterns of concerns
3.  In this dataset, we have 14 variables in total and 12 of which are pure continuous variables
    1.  `RAD`: how easy to access highways from the neighborhood being a [ordinal variable,]{.underline} since it has 24 "levels" ordered by how easy it is to access highway from a neighborhood, by the principle of parsimony, we decided to consider `RAD` as a continuous variables.
    2.  `CHAS`: is a categorical variable representing if land touches Charles River, which is represented as a dummy variables (1 if yes; 0 otherwise)

```{r}
library(psych)
describe(data)
```

## 3. Assess the relationship between potential covariates and Prices

1.  By the research interest in introduction and proposal, we would like to ask what is the relationship between some factors and the median price of owner-occupied homes in Boston neighborhoods, we have `MEDV` as our respond variable
2.  Therefore, we would like to first assess the linear relationship between each of potential covariates and the response variable `MEDV`
    1.  For continuous variables
        1.  we first use `cor()` to look at the correlations between each continuous covariates and `MEDV` and found that most of the correlations are between 0.30 and 0.73 with `DIS` being 0.24 which is a low correlation, signaling a low linear relatedness. `DIS` represents the distance from five Boston employment centers
        2.  we then used scatterplots between each of the continuous covariates and the response variable `MEDV` to visualize the relationship and assess the linearity. Out of which, the plots between `DIS` and our response variable `MEDV` is mostly random with no obvious linear relationship (nor higher power linearity)
        3.  we decided to **drop the variable** `DIS` at this early stage due to its lack of linearity quantitatively and visually. Every other continuous variable has shown a relatively mediate to large correlation, which is meaningful to include in a linear model
    2.  For categorical variable
        1.  we use a boxplot to assess if there is difference between the two categories, and from the boxplot, we see that the Median prices of the neighborhood that is near Charles lake (`CHAS` = 1) is in general higher than the neighborhoods that is not near Charles lake (`CHAS` = 0). So it is meaningful to include `CHAS` in a linear model.

```{r}
library(ggplot2)
library(gridExtra)
# assess the correlation between each potential covariates and 
cor(data)[ncol(cor(data)), ]

## For Continuous Variables 
covariates <- colnames(data)[!colnames(data) %in% c("MEDV", "CHAS")]

# Create scatterplots for each predictor against the response variable
plots <- lapply(covariates, function(pred_var) {
  ggplot(data, aes_string(x = pred_var, y = "MEDV")) + 
    geom_point() +
    labs(
      title = paste("Scatterplot of", pred_var, "vs", "MEDV"),  
      x = pred_var,
      y = "MEDV"
    ) 
})

grid.arrange(grobs = plots, ncol = 4)


## For Categorical Variables 
boxplot2<-ggplot(data,aes(x=as.factor(CHAS),y=MEDV))+
  geom_boxplot() + 
  ylab("Median Price of Owner-Occupied Homes (MEDV)")+
  xlab("CHAS (0 = Not Near Charles River, 1 = Near Charles River)")+
  ggtitle("Relationship Between Whether near Chares River and Median price")
boxplot2

## Drop the variable DIS
data <- data[colnames(data) != "DIS"]
```

## 4. Assess the **Multicollinearity** of the continuous covariates

1.  We suspect that some of the continuous variables might be similar to each other and cause a multicollinearity issue of our linear model, therefore we first do a correlation tables with upper diagonal being the correlation values and the lower diagonal being the scatterplots using `ggpairs` function from `GGally` package.
    1.  from the result tables, we observe that there are some high pair-wise correlation between the continuous covariates, for example, cor(TAX, RAD) = 0.910, cor(NOX, INDUS) = 0.763, cor(NOX, AGE) = 0.737, which signal a potential issue of multicollinearity, therefore we decided to check the Variance Inflation Factor (VIF) in the model (with all potential covariates)
    2.  Then we fit a model with all potential covariates and use `vif` function from the `car` package to get the VIF for each covariates in this model. Surprisingly, none of which has a VIF of \>= 10, which means that multicollinearity is not a very serious issue for the model. Therefore we decided not to drop any covariate and proceed with the current set of covariates

```{r}
suppressMessages(library(GGally))
library(car)
# get continous covariates
con_covariates <- data[!colnames(data) %in% c("MEDV", "CHAS")]

# get correlation values and scaterplots between each pairs 

suppressWarnings(suppressMessages({
  con_covariates <- con_covariates[sapply(con_covariates, is.numeric)]
  plot <- ggpairs(
    con_covariates,
    upper = list(continuous = "cor"),
    lower = list(continuous = "points"),
    diag = list(continuous = "densityDiag"),
    title = "Correlations and Scatterplots Between Continuous Covariates"
  ) + 
    theme(axis.text.x = element_text(angle = -90, hjust = 0))

  print(plot)
}))


## Assess the multicollinearity using VIF for each variables
model1 <- lm(MEDV ~ ., data = data) # full model (without interaction)
vif_values <- vif(model1)
vif_values
```

## 5. Model Selection I: Backward Selection

-   Now after basic checking on summary statistics and multicollinearity issue, we start the Model selection step. we first did a backward selection to select the best backward model based on p-values of the coefficients for each potential covariates

-   We use a alpha value of 0.05 for this process

-   Below is the table representing the covariates that are dropped in the backward selection (in order of being dropped) and their corresponding $p$-values:

| Step | Variable | $p$-value |
|-----:|:--------:|:---------:|
|    1 |    ZN    |   0.648   |
|    2 |  INDUS   |   0.082   |

-   and the final model after backward selection is the model with 10 covariates:

    -   `CRIM` : number of crimes per person

    -   `CHAS`: if land touches Charles River

    -   `NOX`: amount of nitric oxide pollution in the air

    -   `RM`: average number of rooms per home

    -   `AGE`: proportion of owner-occupied units built prior to 1940

    -   `RAD`: how easy to access highways from the neighborhood

    -   `TAX`: full-value property-tax rate per \$10,000

    -   `PTRATIO`: average number of students per teacher

    -   `B`: measure related to proportion of Black residents

    -   `LSTAT`: percentage of population in each neighborhood considered lower income

```{r}
# Create a list of models for backward selection 
backward_models = list()

backward_models[[1]] = lm(MEDV ~ ., data = data) 
summary(backward_models[[1]] )

backward_models[[2]] = lm(MEDV ~ . - ZN, data = data)
summary(backward_models[[2]] )

backward_models[[3]] = lm(MEDV ~ . - ZN - INDUS, data = data)
summary(backward_models[[3]] )

final_backward_model <- lm(MEDV ~ . - ZN - INDUS, data = data)

ggplot() +
  geom_point(aes(x=final_backward_model$fitted.values, y=final_backward_model$residuals)) +
  geom_hline(yintercept = 0) +
  labs(x="Fitted values ($)", y="Residuals ($)", title="Backward model residual plot")
```

## 6. Model Selection II: Forward Selection

-   After backward selection, we decided to run a forward selection to find the best model in forward selection

-   Below is the table representing the covariates that are added in the backward selection (in order of being added) and their corresponding $p$-values:

| Step | Variable | $p$-value |
|-----:|:--------:|:---------:|
|    1 |  LSTAT   |  0.0000   |
|    2 |    RM    |  0.0000   |
|    3 | PTRATIO  |  0.0000   |
|    4 |    B     |  0.0002   |
|    5 |   CHAS   |  0.0003   |

-   and the final model after forward selection is the model with 5 covariates:

    -   `LSTAT`: percentage of population in each neighborhood considered lower income

    -   `RM`: average number of rooms per home

    -   `PTRATIO`: average number of students per teacher

    -   `B`: measure related to proportion of Black residents

    -   `CHAS`: if land touches Charles River

```{r}
response <- "MEDV"  
covariates <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", 
                "RAD", "TAX", "PTRATIO", "B", "LSTAT") 
selected_vars <- c() 
remaining_vars <- covariates  
threshold <- 0.05  
step <- 1

# Loop until no variable meets the threshold
repeat {
  step_results <- data.frame(Variable = character(0), PValue = numeric(0))
  for (var in remaining_vars) {
    formula <- as.formula(paste(response, "~", paste(c(selected_vars, var), collapse = " + ")))
    model <- lm(formula, data = data)
    
    p_value <- summary(model)$coefficients[var, "Pr(>|t|)"]
    step_results <- rbind(step_results, data.frame(Variable = var, PValue = p_value))
  }

  best_var <- step_results$Variable[which.min(step_results$PValue)]
  best_p <- min(step_results$PValue)
  
  if (best_p > threshold) break

  selected_vars <- c(selected_vars, best_var)
  remaining_vars <- setdiff(remaining_vars, best_var)  # Update remaining variables
  cat(sprintf("Step %d: Added variable '%s' with p-value %.4f\n", step, best_var, best_p))
  step <- step + 1
}

# Final model presented with selected variables
final_formula <- as.formula(paste(response, "~", paste(selected_vars, collapse = " + ")))
final_model <- lm(final_formula, data = data)
cat("\nFinal model formula:", deparse(final_formula), "\n")
summary(final_model)

final_forward_model <- lm(MEDV ~ LSTAT + RM + PTRATIO + B + CHAS, data = data)

ggplot() +
  geom_point(aes(x=final_forward_model$fitted.values, y=final_forward_model$residuals)) +
  geom_hline(yintercept = 0) +
  labs(x="Fitted values ($)", y="Residuals ($)", title="Forward model residual plot")
```

## 7. Model Selection III: Model Comparison based on $C_p$ & adj $R^2$

-   Now we have selected one model using backward selection and one model using forward selection. Thus, we are going to compare them using Mallows**'** $C_p$ statistic and adjusted $R^2$ since they have different number of covariates

```{r}
# Calculate Cp for each models
full_model <- backward_models[[1]]
n <- nrow(data)
ms_res <- summary(full_model)$sigma^2 


cp_backward <- sum(residuals(final_backward_model)^2)/(ms_res) - (n - 2 *(10+1))
cp_forward <- sum(residuals(final_forward_model)^2)/(ms_res) - (n - 2 *(5+1))
summary(final_backward_model)
summary(final_forward_model)
adjR_backward <- 0.7048 
adjR_forward <- 0.6934
cp_backward
cp_forward


```

-   Below is a table representing the $C_p$ statistics and adjusted $R^2$ for each model

    | Models               | Mallow's $C_p$ | Adjusted $R^2$ |
    |----------------------|----------------|----------------|
    | Forward Final Model  | 26.13          | 0.6934         |
    | Backward Final Model | 12.24          | 0.7048         |

-   From the table above, we observe that the forward final model has a worse $C_p$ than backward final model, but forward final model has a similar adjusted $R^2$ and fewer parameters (covariates) with 5 covariates (compare to 10 in backward selection model), Therefore we decided to use the forward selection model by the principle of parsimony

## 8. Model fit diagnosis I

1.  to assess the model fit of the chosen model, we first did a residual plot
    1.  from the residual plot, we see that the range of our residuals are too far apart, there are several very large residuals and most of the residuals are around -10 to 10, hard to separate one from the other
2.  to assess the normality assumption violation, we did a Q-Q plot and observe there is a slightly off the straight line especially in the upper end (a skewness), which signals a violation of the normality assumption
3.  to solve the above issue, we consider perform a log-transformation of the response variable `MEDV` to normalize the residual and bring the scale(range) of our residual smaller

```{r}
# residual plot for the final model
res_1 <- resid(final_forward_model)
fitted_1 <- fitted(final_forward_model)
res_plot_1<-ggplot(data,aes(y=res_1,x=fitted_1))+
  geom_point()+
  ylab("Residuals")+
  xlab("Fitted Values")+
  ggtitle("Resisduals vs Fitted Values")+
  geom_hline(yintercept = 0)
res_plot_1



qqnorm(res_1, pch = 1, frame = FALSE)
qqline(res_1, col = "green", lwd = 2)
```

## 9. Data transformation

-   By previous step, we decided to do a log-transformation

-   after the transformation, we observe that our Q-Q plot is more balanced tail, meaning that the normal assumption has been recovered (better

-   after the transformation, some of the large value residual has been reduced and we can

```{r}
log_model <- lm(log(MEDV) ~ LSTAT + RM  + PTRATIO + B + CHAS, 
    data = data)
summary(log_model)

res_log <- resid(log_model)
fitted_log <- fitted(log_model)
res_plot_log<-ggplot(data,aes(y=res_log,x=fitted_log))+
  geom_point()+
  ylab("Residuals")+
  xlab("Fitted Values")+
  ggtitle("Residuals vs Fitted Values")+
  geom_hline(yintercept = 0)
res_plot_log
qqnorm(res_log, pch = 1, frame = FALSE)
qqline(res_log, col = "green", lwd = 2)

```

## Model fit diagnosis II

-   for the second step of model fit diagnosis, we plotted sequential residual to check for violation for independent assumption

    -   from the graph below, the residuals seems to be random and no clear pattern, signalling no obvious violation for independent assumption.

```{r}
plot(head(res_log,99), tail(res_log,99),
     xlab="e(i)", ylab="e(i+1)", cex=1.5, cex.lab=1.5, pch=16)
```

## Current Final Model

```{r}
summary(log_model)
```
